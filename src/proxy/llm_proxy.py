"""
LLM API ä¸­é—´äººä»£ç†
ç”¨äºæ‹¦æˆªå’Œè®°å½•å®¢æˆ·ç«¯ä¸å¤§æ¨¡å‹ API ä¹‹é—´çš„é€šä¿¡
"""
import json
import time
import uuid
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional
import httpx
from fastapi import FastAPI, Request, Response, HTTPException
from fastapi.responses import StreamingResponse
import asyncio
from pydantic import BaseModel

class ProxyConfig(BaseModel):
    """ä»£ç†é…ç½®"""
    target_base_url: str = os.getenv("TARGET_BASE_URL", "https://api.openai.com")
    log_dir: Path = Path("logs/llm_proxy")
    enable_logging: bool = True

class RequestLog(BaseModel):
    """è¯·æ±‚æ—¥å¿—æ¨¡å‹"""
    id: str
    timestamp: datetime
    method: str
    path: str
    headers: Dict[str, str]
    body: Optional[Dict[str, Any]]
    response_status: Optional[int] = None
    response_headers: Optional[Dict[str, str]] = None
    response_body: Optional[Any] = None
    response_chunks: list = []
    duration_ms: Optional[float] = None

class LLMProxy:
    def __init__(self, config: ProxyConfig):
        self.config = config
        self.config.log_dir.mkdir(parents=True, exist_ok=True)
        self.client = httpx.AsyncClient(timeout=60.0)
        print(f"ğŸ¯ ä»£ç†ç›®æ ‡ URL: {self.config.target_base_url}")
        
    async def log_request(self, log_data: RequestLog):
        """ä¿å­˜è¯·æ±‚æ—¥å¿—"""
        if not self.config.enable_logging:
            return
            
        log_file = self.config.log_dir / f"{log_data.id}.json"
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data.model_dump(), f, ensure_ascii=False, indent=2, default=str)
    
    async def proxy_request(self, request: Request) -> Response:
        """ä»£ç†è¯·æ±‚åˆ°ç›®æ ‡ API"""
        request_id = str(uuid.uuid4())
        start_time = time.time()
        
        # è¯»å–è¯·æ±‚ä½“
        body = await request.body()
        body_json = None
        if body:
            try:
                body_json = json.loads(body)
            except:
                body_json = body.decode('utf-8', errors='ignore')
        
        # æ„å»ºè¯·æ±‚æ—¥å¿—
        log_data = RequestLog(
            id=request_id,
            timestamp=datetime.now(),
            method=request.method,
            path=str(request.url.path),
            headers=dict(request.headers),
            body=body_json
        )
        
        # æ„å»ºç›®æ ‡ URL
        target_url = f"{self.config.target_base_url}{request.url.path}"
        if request.url.query:
            target_url += f"?{request.url.query}"
        
        # è½¬å‘è¯·æ±‚å¤´ï¼ˆæ’é™¤ host ç›¸å…³ï¼‰
        headers = {}
        for key, value in request.headers.items():
            if key.lower() not in ['host', 'content-length']:
                headers[key] = value
        
        try:
            # å‘é€è¯·æ±‚åˆ°ç›®æ ‡ API
            response = await self.client.request(
                method=request.method,
                url=target_url,
                headers=headers,
                content=body,
                follow_redirects=True
            )
            
            # è®°å½•å“åº”ä¿¡æ¯
            log_data.response_status = response.status_code
            log_data.response_headers = dict(response.headers)
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æµå¼å“åº”
            is_stream = 'text/event-stream' in response.headers.get('content-type', '')
            
            if is_stream:
                # å¤„ç†æµå¼å“åº”
                async def stream_generator():
                    chunks = []
                    async for chunk in response.aiter_bytes():
                        chunks.append(chunk.decode('utf-8', errors='ignore'))
                        yield chunk
                    
                    # ä¿å­˜æ‰€æœ‰å—
                    log_data.response_chunks = chunks
                    log_data.duration_ms = (time.time() - start_time) * 1000
                    await self.log_request(log_data)
                
                return StreamingResponse(
                    stream_generator(),
                    status_code=response.status_code,
                    headers=dict(response.headers),
                    media_type=response.headers.get('content-type')
                )
            else:
                # å¤„ç†æ™®é€šå“åº”
                response_body = response.text
                try:
                    log_data.response_body = json.loads(response_body)
                except:
                    log_data.response_body = response_body
                
                log_data.duration_ms = (time.time() - start_time) * 1000
                await self.log_request(log_data)
                
                return Response(
                    content=response_body,
                    status_code=response.status_code,
                    headers=dict(response.headers),
                    media_type=response.headers.get('content-type')
                )
                
        except Exception as e:
            log_data.response_status = 500
            log_data.response_body = {"error": str(e)}
            log_data.duration_ms = (time.time() - start_time) * 1000
            await self.log_request(log_data)
            
            raise HTTPException(status_code=500, detail=str(e))

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(title="LLM Proxy Logger")

# å…¨å±€å˜é‡ï¼Œå»¶è¿Ÿåˆå§‹åŒ–
llm_proxy = None

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–ä»£ç†"""
    global llm_proxy
    # æ­¤æ—¶ç¯å¢ƒå˜é‡å·²ç»è®¾ç½®
    proxy_config = ProxyConfig(
        target_base_url=os.getenv("TARGET_BASE_URL", "https://api.openai.com")
    )
    llm_proxy = LLMProxy(proxy_config)

@app.api_route("/{path:path}", methods=["GET", "POST", "PUT", "DELETE", "PATCH", "OPTIONS"])
async def proxy_endpoint(request: Request, path: str):
    """é€šç”¨ä»£ç†ç«¯ç‚¹"""
    if llm_proxy is None:
        raise HTTPException(status_code=500, detail="Proxy not initialized")
    return await llm_proxy.proxy_request(request)

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­æ—¶æ¸…ç†èµ„æº"""
    if llm_proxy:
        await llm_proxy.client.aclose() 